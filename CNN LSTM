{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:21:25.229095Z","iopub.execute_input":"2025-04-07T09:21:25.229559Z","iopub.status.idle":"2025-04-07T09:21:27.813334Z","shell.execute_reply.started":"2025-04-07T09:21:25.229525Z","shell.execute_reply":"2025-04-07T09:21:27.812434Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/faceforensics/FF++/fake/02_13__exit_phone_room__CP5HFV3K.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__talking_against_wall__ZC2KYASW.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__outside_talking_pan_laughing__Y11NT1YX.mp4\n/kaggle/input/faceforensics/FF++/fake/07_26__walking_down_street_outside_angry__FGNGC2GT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__hugging_happy__7NGMD8FT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_09__walk_down_hall_angry__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/01_12__outside_talking_pan_laughing__TNI7KUZ6.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__podium_speech_happy__6PHZRQ4H.mp4\n/kaggle/input/faceforensics/FF++/fake/07_02__walking_down_street_outside_angry__O4SXNLRL.mp4\n/kaggle/input/faceforensics/FF++/fake/08_05__walk_down_hall_angry__FBICSP2C.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__outside_talking_pan_laughing__DEA1TCLN.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__talking_against_wall__JZUXXFRB.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__podium_speech_happy__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/09_07__kitchen_pan__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/01_02__walk_down_hall_angry__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__talking_angry_couch__MKZTXQ2T.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_angry_couch__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__secret_conversation__CP5HFV3K.mp4\n/kaggle/input/faceforensics/FF++/fake/02_03__walking_down_street_outside_angry__QH3Y0IG0.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__outside_talking_still_laughing__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__hugging_happy__BKLOCI1M.mp4\n/kaggle/input/faceforensics/FF++/fake/05_08__talking_against_wall__PRBCE28Z.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__outside_talking_still_laughing__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__secret_conversation__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walk_down_hall_angry__U7DEOZNV.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__walking_and_outside_surprised__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__kitchen_pan__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__secret_conversation__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/07_02__talking_angry_couch__1H07DFQJ.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__kitchen_pan__AIOM1U5V.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walk_down_hall_angry__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__secret_conversation__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__walk_down_hall_angry__P1L5PF4I.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__walking_outside_cafe_disgusted__FAFWDR4W.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__podium_speech_happy__DEA1TCLN.mp4\n/kaggle/input/faceforensics/FF++/fake/07_20__outside_talking_pan_laughing__KV6Q7D6C.mp4\n/kaggle/input/faceforensics/FF++/fake/06_04__walking_outside_cafe_disgusted__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/04_21__walking_down_street_outside_angry__5Y31RZP8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__walking_outside_cafe_disgusted__F0YYEA5W.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walking_down_street_outside_angry__6KUOFMZW.mp4\n/kaggle/input/faceforensics/FF++/fake/10_19__kitchen_still__IDX76N5R.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__outside_talking_pan_laughing__2YSYT2N3.mp4\n/kaggle/input/faceforensics/FF++/fake/04_06__kitchen_still__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_02__podium_speech_happy__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/03_27__walk_down_hall_angry__IL675GCI.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__walking_outside_cafe_disgusted__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walking_and_outside_surprised__1VMZUH1W.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__hugging_happy__ISF9SP4G.mp4\n/kaggle/input/faceforensics/FF++/fake/09_18__outside_talking_pan_laughing__3VP8836C.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__outside_talking_still_laughing__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/03_27__walking_down_indoor_hall_disgust__IL675GCI.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__meeting_serious__T3MZOI8X.mp4\n/kaggle/input/faceforensics/FF++/fake/07_14__talking_against_wall__P9QFO50U.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__walking_and_outside_surprised__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/09_26__talking_against_wall__C3K20JOL.mp4\n/kaggle/input/faceforensics/FF++/fake/02_14__outside_talking_pan_laughing__3IUBEKCT.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__outside_talking_pan_laughing__3K21NFNM.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__walk_down_hall_angry__U9WZI5LK.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__CDSNLDQ8.mp4\n/kaggle/input/faceforensics/FF++/fake/02_27__hugging_happy__GVFLSZD5.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__kitchen_pan__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__outside_talking_pan_laughing__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/09_01__talking_against_wall__O8HNNX43.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__outside_talking_pan_laughing__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__outside_talking_pan_laughing__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/07_25__walk_down_hall_angry__PAE9HCA8.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__podium_speech_happy__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/01_21__walk_down_hall_angry__03X7CELV.mp4\n/kaggle/input/faceforensics/FF++/fake/03_11__exit_phone_room__P08VGHTA.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__outside_talking_still_laughing__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/06_04__outside_talking_still_laughing__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__podium_speech_happy__O5X0AWR9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_06__meeting_serious__83ABVHC3.mp4\n/kaggle/input/faceforensics/FF++/fake/05_17__hugging_happy__YTJYYDO9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__PWXXULHR.mp4\n/kaggle/input/faceforensics/FF++/fake/02_03__walking_outside_cafe_disgusted__QH3Y0IG0.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__walking_down_street_outside_angry__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/09_25__walking_down_street_outside_angry__5041ODBN.mp4\n/kaggle/input/faceforensics/FF++/fake/07_12__outside_talking_still_laughing__0VN0A2T3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_06__podium_speech_happy__83ABVHC3.mp4\n/kaggle/input/faceforensics/FF++/fake/02_09__kitchen_pan__HIH8YA82.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__talking_angry_couch__WPT3Z2KN.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__talking_angry_couch__0IRM5ADD.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__exit_phone_room__B62WCGUN.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__podium_speech_happy__3K21NFNM.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__meeting_serious__9OM3VE0Y.mp4\n/kaggle/input/faceforensics/FF++/fake/02_27__walk_down_hall_angry__78M8S6M6.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__walk_down_hall_angry__OSXCUOHX.mp4\n/kaggle/input/faceforensics/FF++/fake/04_26__outside_talking_still_laughing__WX836VLY.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walking_down_street_outside_angry__O4SXNLRL.mp4\n/kaggle/input/faceforensics/FF++/fake/04_07__exit_phone_room__ITC0C48B.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walk_down_hall_angry__6KUOFMZW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__walk_down_hall_angry__LH0KWJKM.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__kitchen_pan__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__secret_conversation__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/01_20__outside_talking_pan_laughing__OTGHOG4Z.mp4\n/kaggle/input/faceforensics/FF++/fake/03_18__walking_outside_cafe_disgusted__22UBC0BS.mp4\n/kaggle/input/faceforensics/FF++/fake/07_27__walking_down_street_outside_angry__3RH5PR6S.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__outside_talking_still_laughing__DNUJD8M2.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__podium_speech_happy__2YSYT2N3.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__exit_phone_room__OXMEEFUQ.mp4\n/kaggle/input/faceforensics/FF++/fake/09_01__walk_down_hall_angry__6TSGVLHA.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__walking_down_indoor_hall_disgust__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__outside_talking_still_laughing__K7KXUHMU.mp4\n/kaggle/input/faceforensics/FF++/fake/02_12__podium_speech_happy__9D2ZHEKW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__exit_phone_room__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_03__talking_against_wall__4I8LRXWF.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__talking_against_wall__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__secret_conversation__4OJNJLOO.mp4\n/kaggle/input/faceforensics/FF++/fake/01_20__outside_talking_still_laughing__FW94AIMJ.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__walking_and_outside_surprised__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/04_18__kitchen_still__NAXINA1N.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__outside_talking_pan_laughing__OXMEEFUQ.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__exit_phone_room__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_20__talking_against_wall__KV6Q7D6C.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_down_indoor_hall_disgust__PWXXULHR.mp4\n/kaggle/input/faceforensics/FF++/fake/07_13__walking_outside_cafe_disgusted__RVQCPCJF.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__hugging_happy__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__exit_phone_room__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walk_down_hall_angry__TN2CWM3K.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__exit_phone_room__0XUW13RW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_26__kitchen_still__L5BVR5L9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_11__talking_against_wall__P08VGHTA.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__meeting_serious__1JCLEEBQ.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__meeting_serious__V53E3RVB.mp4\n/kaggle/input/faceforensics/FF++/fake/09_13__kitchen_pan__21H6XSPE.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__outside_talking_still_laughing__0XUW13RW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_02__walk_down_hall_angry__37DH75GQ.mp4\n/kaggle/input/faceforensics/FF++/fake/02_21__kitchen_pan__Z0XHPQAR.mp4\n/kaggle/input/faceforensics/FF++/fake/03_04__outside_talking_pan_laughing__T04P6ELC.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__secret_conversation__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__podium_speech_happy__DG8ITQO3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__outside_talking_still_laughing__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__exit_phone_room__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/05_28__exit_phone_room__U9LRLJ6N.mp4\n/kaggle/input/faceforensics/FF++/fake/03_18__walking_and_outside_surprised__8HNSXOBW.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__kitchen_pan__DNUJD8M2.mp4\n/kaggle/input/faceforensics/FF++/fake/07_09__outside_talking_still_laughing__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__outside_talking_still_laughing__M36D0OJT.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_against_wall__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/09_03__kitchen_pan__8DTEGQ54.mp4\n/kaggle/input/faceforensics/FF++/fake/10_22__kitchen_still__EHARPYBT.mp4\n/kaggle/input/faceforensics/FF++/fake/08_28__outside_talking_pan_laughing__8BC35RFU.mp4\n/kaggle/input/faceforensics/FF++/fake/02_21__talking_angry_couch__Z0XHPQAR.mp4\n/kaggle/input/faceforensics/FF++/fake/03_01__walking_and_outside_surprised__JZUXXFRB.mp4\n/kaggle/input/faceforensics/FF++/fake/06_20__kitchen_pan__6SUW7063.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__walking_down_street_outside_angry__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__talking_angry_couch__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/02_01__secret_conversation__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/03_04__talking_angry_couch__T04P6ELC.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__walking_down_street_outside_angry__21JTDDEL.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__outside_talking_pan_laughing__SB6PMCO0.mp4\n/kaggle/input/faceforensics/FF++/fake/06_11__walking_outside_cafe_disgusted__MX659QU8.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walking_down_street_outside_angry__9TDCEK1Q.mp4\n/kaggle/input/faceforensics/FF++/fake/07_15__hugging_happy__9Z2MLKVX.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__walking_down_street_outside_angry__H0VQHGS3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_09__outside_talking_still_laughing__RCETIXYL.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__kitchen_pan__98NUQ3E6.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__talking_angry_couch__GH8TGTBS.mp4\n/kaggle/input/faceforensics/FF++/fake/03_09__secret_conversation__RCETIXYL.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__exit_phone_room__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__walking_outside_cafe_disgusted__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/07_26__walk_down_hall_angry__FGNGC2GT.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__kitchen_still__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/09_13__outside_talking_pan_laughing__LPT427RY.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__podium_speech_happy__Q9NSXM88.mp4\n/kaggle/input/faceforensics/FF++/fake/06_27__walking_and_outside_surprised__O7L5Z9U8.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__podium_speech_happy__0VR4Y891.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_angry_couch__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/05_28__kitchen_still__W3J028UG.mp4\n/kaggle/input/faceforensics/FF++/fake/09_26__walk_down_hall_angry__QSE5A0GF.mp4\n/kaggle/input/faceforensics/FF++/fake/10_22__kitchen_pan__EHARPYBT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__talking_against_wall__MKU99DVX.mp4\n/kaggle/input/faceforensics/FF++/fake/07_14__exit_phone_room__P9QFO50U.mp4\n/kaggle/input/faceforensics/FF++/fake/06_26__kitchen_pan__L5BVR5L9.mp4\n/kaggle/input/faceforensics/FF++/fake/02_25__talking_against_wall__Z7FQ69VP.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__walking_down_street_outside_angry__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/04_12__walking_down_street_outside_angry__96TQKDFJ.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__exit_phone_room__K7KXUHMU.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__kitchen_pan__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walk_down_hall_angry__6PHZRQ4H.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__kitchen_pan__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__walking_and_outside_surprised__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__outside_talking_pan_laughing__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/04_07__kitchen_pan__XRK7FGZX.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__podium_speech_happy__480LQD1C.mp4\n/kaggle/input/faceforensics/FF++/fake/08_16__podium_speech_happy__8Q7JCS95.mp4\n/kaggle/input/faceforensics/FF++/fake/05_17__walking_down_street_outside_angry__M3H96PDQ.mp4\n/kaggle/input/faceforensics/FF++/fake/09_03__talking_against_wall__8DTEGQ54.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__kitchen_still__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__walking_down_indoor_hall_disgust__U6MDWIHG.mp4\n/kaggle/input/faceforensics/FF++/fake/06_27__talking_angry_couch__JOG5PB18.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__meeting_serious__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/02_09__exit_phone_room__HIH8YA82.mp4\n/kaggle/input/faceforensics/FF++/real/08__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/08__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/05__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/06__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/02__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/01__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/03__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/05__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/08__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/03__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/16__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/11__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/09__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/13__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/07__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/04__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/06__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/07__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/14__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/07__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/09__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/09__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/01__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/12__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/07__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/09__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/06__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/11__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/10__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/14__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/13__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/13__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/07__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/11__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/09__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/11__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/08__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/14__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/08__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/11__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/01__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/11__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/02__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/08__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/05__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/01__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/05__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/02__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/05__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/02__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/05__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/03__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/13__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/04__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/02__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/02__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/14__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/10__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/04__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/11__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/01__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/12__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/13__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/14__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/05__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/14__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/09__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/15__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/01__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/04__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/12__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/15__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/08__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/03__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/10__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/11__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/03__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/15__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/15__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/03__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/15__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/02__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/04__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/13__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/08__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/15__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/07__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/09__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/02__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/09__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/11__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/07__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/09__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/15__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/08__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/14__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/12__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/08__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/01__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/02__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/03__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/15__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/03__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/05__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/13__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/02__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/04__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/12__exit_phone_room.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T07:00:57.244009Z","iopub.execute_input":"2025-03-24T07:00:57.244430Z","iopub.status.idle":"2025-03-24T07:01:29.431485Z","shell.execute_reply.started":"2025-03-24T07:00:57.244397Z","shell.execute_reply":"2025-03-24T07:01:29.429822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom conv2d cnn\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Limit GPU Memory Usage for Kaggle\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Step 1: Get Dataset Paths (FIXED PATH)\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"  # ✅ Corrected dataset path\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' video folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\n# ✅ Step 2: Extract Frames from Videos\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"❌ Error opening video file: {video_path}\")\n        return []\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n\n    saved_frames = []\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count in frames_to_extract:\n            try:\n                frame = cv2.resize(frame, img_size)\n                frame_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n                cv2.imwrite(frame_path, frame)\n                saved_frames.append(frame_path)\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n\n        frame_count += 1\n\n    cap.release()\n    return saved_frames\n\n# ✅ Step 3: Process Dataset (UPDATED FOR FF++)\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    frame_data = []\n\n    def process_video_batch(video_dir, label):\n        batch_frames = []\n        for video_file in os.listdir(video_dir)[:max_videos]:\n            if video_file.endswith(('.mp4', '.avi')):\n                video_path = os.path.join(video_dir, video_file)\n                video_output_dir = os.path.join(output_dir, label, os.path.splitext(video_file)[0])\n                frames = extract_frames(video_path, video_output_dir)\n                for frame in frames:\n                    batch_frames.append({'path': frame, 'label': label})\n                clear_memory()\n        return batch_frames\n\n    frame_data += process_video_batch(real_dir, \"real\")\n    frame_data += process_video_batch(fake_dir, \"fake\")\n\n    return pd.DataFrame(frame_data)\n\n# ✅ Step 4: Define CNN Model\ndef create_cnn_model(input_shape=(96, 96, 3)):\n    model = Sequential([\n        Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Step 5: Main Function\ndef main():\n    print(\"🚀 Starting Deepfake Detection on FF++ Dataset...\")\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir, max_videos=50)\n    datagen = ImageDataGenerator(rescale=1./255)\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n\n    # ✅ Convert labels to strings\n    train_df['label'] = train_df['label'].astype(str)\n    val_df['label'] = val_df['label'].astype(str)\n\n    train_generator = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary')\n    val_generator = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary')\n\n    model = create_cnn_model()\n    model.fit(train_generator, validation_data=val_generator, epochs=10)\n\n# ✅ Run Main Function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T17:11:49.107623Z","iopub.execute_input":"2025-03-23T17:11:49.107976Z","iopub.status.idle":"2025-03-23T17:19:19.550153Z","shell.execute_reply.started":"2025-03-23T17:11:49.107951Z","shell.execute_reply":"2025-03-23T17:19:19.549210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\ndef get_dataset_paths():\n    base_dir = \"/kaggle/input/faceforensics/FF++\"  # Change path if needed\n    real_dir = os.path.join(base_dir, \"real\")\n    fake_dir = os.path.join(base_dir, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Could not locate 'real' and 'fake' video folders!\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n\n    return real_dir, fake_dir\ndef extract_frames(video_path, output_folder, max_frames=10):\n    os.makedirs(output_folder, exist_ok=True)\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    success, frame = cap.read()\n\n    extracted_frames = []\n    \n    while success and frame_count < max_frames:\n        frame_path = os.path.join(output_folder, f\"{os.path.basename(video_path)}_frame{frame_count}.jpg\")\n        cv2.imwrite(frame_path, frame)\n        extracted_frames.append(frame_path)\n        frame_count += 1\n        success, frame = cap.read()\n\n    cap.release()\n    return extracted_frames\ndef prepare_dataset():\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n    data = []\n\n    for label, folder in [(\"real\", real_dir), (\"fake\", fake_dir)]:\n        for video in tqdm(os.listdir(folder), desc=f\"Processing {label} videos\"):\n            video_path = os.path.join(folder, video)\n            frame_output = os.path.join(output_dir, label, video.split('.')[0])\n\n            frame_paths = extract_frames(video_path, frame_output)\n\n            for frame_path in frame_paths:\n                if os.path.exists(frame_path):\n                    data.append((frame_path, label))\n\n    df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n    \n    # 🔹 Fix missing values issue\n    df.dropna(inplace=True)\n    \n    print(f\"✅ Total Frames: {len(df)} (Real: {sum(df['label'] == 'real')}, Fake: {sum(df['label'] == 'fake')})\")\n\n    return df\ndef create_cnn_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\ndef train_model(frame_df):\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=15,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,\n        zoom_range=0.2\n    )\n\n    # 🔹 Fix missing labels issue\n    frame_df.dropna(subset=[\"label\"], inplace=True)\n\n    # 🔹 Convert labels to string (\"real\", \"fake\")\n    frame_df[\"label\"] = frame_df[\"label\"].astype(str)\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df[\"label\"])\n\n    train_generator = datagen.flow_from_dataframe(\n        train_df, x_col=\"path\", y_col=\"label\",\n        target_size=(96, 96), batch_size=16, class_mode=\"binary\"\n    )\n\n    val_generator = datagen.flow_from_dataframe(\n        val_df, x_col=\"path\", y_col=\"label\",\n        target_size=(96, 96), batch_size=16, class_mode=\"binary\"\n    )\n\n    # ✅ Define CNN Model\n    model = create_cnn_model()\n\n    # ✅ Add Early Stopping & Learning Rate Reduction\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n\n    # ✅ Train the model\n    model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=50,  # Increased for better convergence\n        callbacks=[early_stopping, reduce_lr]\n    )\n\n    return model\nif __name__ == \"__main__\":\n    print(\"🚀 Starting Deepfake Detection on FaceForensics++ Dataset...\")\n    \n    # ✅ Prepare dataset\n    frame_df = prepare_dataset()\n    \n    # ✅ Train model\n    model = train_model(frame_df)\n    \n    # ✅ Save trained model\n    model.save(\"/kaggle/working/deepfake_detector.h5\")\n    print(\"✅ Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T10:28:49.028103Z","iopub.execute_input":"2025-04-07T10:28:49.028558Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Deepfake Detection on FaceForensics++ Dataset...\n✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\n","output_type":"stream"},{"name":"stderr","text":"Processing real videos: 100%|██████████| 200/200 [00:37<00:00,  5.28it/s]\nProcessing fake videos: 100%|██████████| 200/200 [00:37<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Total Frames: 4000 (Real: 2000, Fake: 2000)\nFound 3200 validated image filenames belonging to 2 classes.\nFound 800 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 351ms/step - accuracy: 0.5118 - loss: 0.7073 - val_accuracy: 0.5700 - val_loss: 0.6753 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 350ms/step - accuracy: 0.5497 - loss: 0.6816 - val_accuracy: 0.5700 - val_loss: 0.6769 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 342ms/step - accuracy: 0.5799 - loss: 0.6678 - val_accuracy: 0.5788 - val_loss: 0.6545 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 343ms/step - accuracy: 0.5929 - loss: 0.6559 - val_accuracy: 0.6488 - val_loss: 0.6303 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m154/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m14s\u001b[0m 318ms/step - accuracy: 0.5995 - loss: 0.6607","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# squeeze and Excitation attention mechanism + Custom CNN","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n                                     GlobalAveragePooling2D, Input, Reshape, Multiply, \n                                     BatchNormalization, Activation)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Limit GPU Memory Usage\n\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Get Dataset Paths\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Could not locate 'real' and 'fake' folders\")\n\n    return real_dir, fake_dir\n\n# ✅ Extract Frames\n\ndef extract_frames(video_path, output_dir, max_frames=20, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    frames_to_extract = np.linspace(0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1, \n                                    num=max_frames, dtype=int)\n    frame_paths = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count in frames_to_extract:\n            frame = cv2.resize(frame, img_size)\n            path = os.path.join(output_dir, f\"frame_{count}.jpg\")\n            cv2.imwrite(path, frame)\n            frame_paths.append({'path': path, 'label': os.path.basename(os.path.dirname(video_path))})\n        count += 1\n    cap.release()\n    return frame_paths\n\n# ✅ Process Dataset\n\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    all_frames = []\n\n    def process_videos(video_dir, label):\n        for video in os.listdir(video_dir)[:max_videos]:\n            if video.endswith(('.mp4', '.avi')):\n                path = os.path.join(video_dir, video)\n                out_dir = os.path.join(output_dir, label, os.path.splitext(video)[0])\n                all_frames.extend(extract_frames(path, out_dir))\n                clear_memory()\n\n    process_videos(real_dir, \"real\")\n    process_videos(fake_dir, \"fake\")\n    return pd.DataFrame(all_frames)\n\n# ✅ SE Block\n\ndef se_block(input_tensor, ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\n# ✅ Model\n\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.0010), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Main\n\ndef main():\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir)\n    if frame_df.empty:\n        raise ValueError(\"❌ No data processed\")\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n    train_df['label'] = train_df['label'].astype(str)\n    val_df['label'] = val_df['label'].astype(str)\n\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n    )\n\n    train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label',\n                                            target_size=(96, 96), batch_size=16, class_mode='binary')\n    val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label',\n                                          target_size=(96, 96), batch_size=16, class_mode='binary')\n\n    model = create_se_cnn_model()\n    model.summary()\n \n    callbacks = [\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    ]\n\n    model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=callbacks)\n    model.save(\"/kaggle/working/se_cnnfinetunned1.h5\")\n    print(\"✅ Model saved successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:02:09.567829Z","iopub.execute_input":"2025-04-14T21:02:09.568120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# stratified\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n                                     GlobalAveragePooling2D, Input, Reshape, Multiply, \n                                     BatchNormalization, Activation)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Limit GPU Memory Usage\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Get Dataset Paths\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Could not locate 'real' and 'fake' folders\")\n\n    return real_dir, fake_dir\n\n# ✅ Extract Frames\ndef extract_frames(video_path, output_dir, max_frames=20, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    frames_to_extract = np.linspace(0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1, \n                                    num=max_frames, dtype=int)\n    frame_paths = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count in frames_to_extract:\n            frame = cv2.resize(frame, img_size)\n            path = os.path.join(output_dir, f\"frame_{count}.jpg\")\n            cv2.imwrite(path, frame)\n            frame_paths.append({'path': path, 'label': os.path.basename(os.path.dirname(video_path))})\n        count += 1\n    cap.release()\n    return frame_paths\n\n# ✅ Process Dataset with Controlled Split\ndef process_dataset_balanced(real_dir, fake_dir, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    all_frames = []\n\n    real_videos = [f for f in os.listdir(real_dir) if f.endswith(('.mp4', '.avi'))]\n    fake_videos = [f for f in os.listdir(fake_dir) if f.endswith(('.mp4', '.avi'))]\n\n    random.shuffle(real_videos)\n    random.shuffle(fake_videos)\n\n    test_real = real_videos[:10]\n    test_fake = fake_videos[:10]\n    trainval_real = real_videos[10:190]\n    trainval_fake = fake_videos[10:190]\n\n    def process_videos(video_list, label, split):\n        for video in video_list:\n            path = os.path.join(real_dir if label == 'real' else fake_dir, video)\n            out_dir = os.path.join(output_dir, split, label, os.path.splitext(video)[0])\n            all_frames.extend(extract_frames(path, out_dir))\n\n    process_videos(trainval_real, \"real\", \"trainval\")\n    process_videos(trainval_fake, \"fake\", \"trainval\")\n    process_videos(test_real, \"real\", \"test\")\n    process_videos(test_fake, \"fake\", \"test\")\n\n    return pd.DataFrame(all_frames)\n\n# ✅ SE Block\ndef se_block(input_tensor, ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\n# ✅ Model\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Main\ndef main():\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset_balanced(real_dir, fake_dir, output_dir)\n    if frame_df.empty:\n        raise ValueError(\"❌ No data processed\")\n\n    trainval_df = frame_df[frame_df['path'].str.contains(\"trainval\")]\n    test_df = frame_df[frame_df['path'].str.contains(\"test\")]\n\n    train_df, val_df = train_test_split(trainval_df, test_size=0.2, stratify=trainval_df['label'])\n    for df in [train_df, val_df, test_df]:\n        df['label'] = df['label'].astype(str)\n\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n    )\n\n    train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label',\n                                            target_size=(96, 96), batch_size=16, class_mode='binary')\n    val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label',\n                                          target_size=(96, 96), batch_size=16, class_mode='binary')\n    test_gen = datagen.flow_from_dataframe(test_df, x_col='path', y_col='label',\n                                           target_size=(96, 96), batch_size=16, class_mode='binary', shuffle=False)\n\n    model = create_se_cnn_model()\n    model.summary()\n\n    callbacks = [\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    ]\n\n    history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=callbacks)\n    model.save(\"/kaggle/working/se_cnn_final_model.h5\")\n    print(\"✅ Model saved successfully!\")\n\n    # ✅ Evaluate on test set\n    test_loss, test_acc = model.evaluate(test_gen)\n    print(f\"\\n✅ Final Test Accuracy: {test_acc:.4f}\")\n    print(f\"✅ Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n    print(f\"✅ Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:10:42.159410Z","iopub.execute_input":"2025-04-09T14:10:42.159768Z","iopub.status.idle":"2025-04-09T14:46:21.254462Z","shell.execute_reply.started":"2025-04-09T14:10:42.159740Z","shell.execute_reply":"2025-04-09T14:46:21.253754Z"}},"outputs":[{"name":"stdout","text":"Found 5760 validated image filenames belonging to 2 classes.\nFound 1440 validated image filenames belonging to 2 classes.\nFound 400 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-c7a021ef7b20>:149: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['label'] = df['label'].astype(str)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m17\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m32\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m128\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m66\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m96\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│                           │                        │                │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m256\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m260\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m320\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│                           │                        │                │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m8,320\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│                           │                        │                │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│                           │                        │                │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,272\u001b[0m (129.97 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,272</span> (129.97 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,048\u001b[0m (129.09 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,048</span> (129.09 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 56ms/step - accuracy: 0.5107 - loss: 0.7101 - val_accuracy: 0.5000 - val_loss: 0.7010 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.5463 - loss: 0.6931 - val_accuracy: 0.5417 - val_loss: 0.6878 - learning_rate: 5.0000e-04\nEpoch 3/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.5538 - loss: 0.6887 - val_accuracy: 0.5458 - val_loss: 0.6976 - learning_rate: 5.0000e-04\nEpoch 4/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.5866 - loss: 0.6786 - val_accuracy: 0.5715 - val_loss: 0.6791 - learning_rate: 5.0000e-04\nEpoch 5/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.5883 - loss: 0.6719 - val_accuracy: 0.5875 - val_loss: 0.6855 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6024 - loss: 0.6675 - val_accuracy: 0.6160 - val_loss: 0.6644 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6100 - loss: 0.6569 - val_accuracy: 0.5972 - val_loss: 0.6748 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.6299 - loss: 0.6474 - val_accuracy: 0.5868 - val_loss: 0.6976 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m353/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6428 - loss: 0.6382\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6429 - loss: 0.6382 - val_accuracy: 0.5514 - val_loss: 0.8087 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6558 - loss: 0.6278 - val_accuracy: 0.6097 - val_loss: 0.6651 - learning_rate: 2.5000e-04\nEpoch 11/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6663 - loss: 0.6179 - val_accuracy: 0.6187 - val_loss: 0.6697 - learning_rate: 2.5000e-04\nEpoch 12/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6670 - loss: 0.6179 - val_accuracy: 0.6562 - val_loss: 0.6210 - learning_rate: 2.5000e-04\nEpoch 13/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6567 - loss: 0.6113 - val_accuracy: 0.6465 - val_loss: 0.6193 - learning_rate: 2.5000e-04\nEpoch 14/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6790 - loss: 0.6005 - val_accuracy: 0.6778 - val_loss: 0.6003 - learning_rate: 2.5000e-04\nEpoch 15/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6740 - loss: 0.6019 - val_accuracy: 0.5653 - val_loss: 0.7829 - learning_rate: 2.5000e-04\nEpoch 16/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6743 - loss: 0.6081 - val_accuracy: 0.6285 - val_loss: 0.6336 - learning_rate: 2.5000e-04\nEpoch 17/50\n\u001b[1m355/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6746 - loss: 0.5978\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6747 - loss: 0.5978 - val_accuracy: 0.6493 - val_loss: 0.6051 - learning_rate: 2.5000e-04\nEpoch 18/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6925 - loss: 0.5863 - val_accuracy: 0.6792 - val_loss: 0.5999 - learning_rate: 1.2500e-04\nEpoch 19/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7027 - loss: 0.5747 - val_accuracy: 0.6833 - val_loss: 0.6083 - learning_rate: 1.2500e-04\nEpoch 20/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7006 - loss: 0.5764 - val_accuracy: 0.6639 - val_loss: 0.6136 - learning_rate: 1.2500e-04\nEpoch 21/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7116 - loss: 0.5663 - val_accuracy: 0.6847 - val_loss: 0.5935 - learning_rate: 1.2500e-04\nEpoch 22/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6962 - loss: 0.5820 - val_accuracy: 0.6840 - val_loss: 0.5933 - learning_rate: 1.2500e-04\nEpoch 23/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7042 - loss: 0.5668 - val_accuracy: 0.6354 - val_loss: 0.6212 - learning_rate: 1.2500e-04\nEpoch 24/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6976 - loss: 0.5714 - val_accuracy: 0.6868 - val_loss: 0.5912 - learning_rate: 1.2500e-04\nEpoch 25/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7075 - loss: 0.5664 - val_accuracy: 0.6958 - val_loss: 0.5742 - learning_rate: 1.2500e-04\nEpoch 26/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7096 - loss: 0.5669 - val_accuracy: 0.7042 - val_loss: 0.5722 - learning_rate: 1.2500e-04\nEpoch 27/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.6999 - loss: 0.5731 - val_accuracy: 0.6562 - val_loss: 0.6046 - learning_rate: 1.2500e-04\nEpoch 28/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.7120 - loss: 0.5646 - val_accuracy: 0.6444 - val_loss: 0.6351 - learning_rate: 1.2500e-04\nEpoch 29/50\n\u001b[1m354/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7045 - loss: 0.5595\nEpoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7044 - loss: 0.5596 - val_accuracy: 0.6632 - val_loss: 0.5985 - learning_rate: 1.2500e-04\nEpoch 30/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7174 - loss: 0.5613 - val_accuracy: 0.6896 - val_loss: 0.5883 - learning_rate: 6.2500e-05\nEpoch 31/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7100 - loss: 0.5638 - val_accuracy: 0.6750 - val_loss: 0.5936 - learning_rate: 6.2500e-05\nEpoch 32/50\n\u001b[1m355/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7007 - loss: 0.5640\nEpoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7008 - loss: 0.5639 - val_accuracy: 0.6924 - val_loss: 0.5808 - learning_rate: 6.2500e-05\nEpoch 33/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7175 - loss: 0.5584 - val_accuracy: 0.7111 - val_loss: 0.5605 - learning_rate: 3.1250e-05\nEpoch 34/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7246 - loss: 0.5395 - val_accuracy: 0.6979 - val_loss: 0.5650 - learning_rate: 3.1250e-05\nEpoch 35/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7136 - loss: 0.5529 - val_accuracy: 0.7132 - val_loss: 0.5590 - learning_rate: 3.1250e-05\nEpoch 36/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7047 - loss: 0.5579 - val_accuracy: 0.7014 - val_loss: 0.5667 - learning_rate: 3.1250e-05\nEpoch 37/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7119 - loss: 0.5571 - val_accuracy: 0.7160 - val_loss: 0.5585 - learning_rate: 3.1250e-05\nEpoch 38/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.7086 - loss: 0.5597 - val_accuracy: 0.7097 - val_loss: 0.5630 - learning_rate: 3.1250e-05\nEpoch 39/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7177 - loss: 0.5564 - val_accuracy: 0.6854 - val_loss: 0.5918 - learning_rate: 3.1250e-05\nEpoch 40/50\n\u001b[1m353/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7255 - loss: 0.5500\nEpoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.7255 - loss: 0.5500 - val_accuracy: 0.7049 - val_loss: 0.5649 - learning_rate: 3.1250e-05\nEpoch 41/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.7267 - loss: 0.5434 - val_accuracy: 0.7083 - val_loss: 0.5585 - learning_rate: 1.5625e-05\nEpoch 42/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.7237 - loss: 0.5516 - val_accuracy: 0.7076 - val_loss: 0.5630 - learning_rate: 1.5625e-05\nEpoch 43/50\n\u001b[1m355/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7241 - loss: 0.5456\nEpoch 43: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.7240 - loss: 0.5457 - val_accuracy: 0.6972 - val_loss: 0.5628 - learning_rate: 1.5625e-05\nEpoch 44/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7243 - loss: 0.5520 - val_accuracy: 0.7083 - val_loss: 0.5636 - learning_rate: 7.8125e-06\nEpoch 45/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.7324 - loss: 0.5459 - val_accuracy: 0.7083 - val_loss: 0.5625 - learning_rate: 7.8125e-06\nEpoch 46/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7180 - loss: 0.5500 - val_accuracy: 0.7083 - val_loss: 0.5568 - learning_rate: 7.8125e-06\nEpoch 47/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7208 - loss: 0.5500 - val_accuracy: 0.7167 - val_loss: 0.5580 - learning_rate: 7.8125e-06\nEpoch 48/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.7151 - loss: 0.5498 - val_accuracy: 0.7021 - val_loss: 0.5646 - learning_rate: 7.8125e-06\nEpoch 49/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7217 - loss: 0.5454 - val_accuracy: 0.7132 - val_loss: 0.5531 - learning_rate: 7.8125e-06\nEpoch 50/50\n\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7185 - loss: 0.5601 - val_accuracy: 0.7188 - val_loss: 0.5488 - learning_rate: 7.8125e-06\n✅ Model saved successfully!\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.7449 - loss: 0.6001\n\n✅ Final Test Accuracy: 0.8075\n✅ Final Training Accuracy: 0.7201\n✅ Final Validation Accuracy: 0.7188\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ✅ Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n                                     GlobalAveragePooling2D, Input, Reshape, Multiply, \n                                     BatchNormalization, Activation)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\n# ✅ Limit GPU Memory Usage\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Dataset Paths\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Could not locate 'real' and 'fake' folders\")\n\n    return real_dir, fake_dir\n\n# ✅ Randomized Frame Extraction\ndef extract_frames(video_path, output_dir, max_frames=20, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_ids = sorted(random.sample(range(total_frames), min(max_frames, total_frames)))\n    \n    frame_paths = []\n    count = 0\n    i = 0\n    while cap.isOpened() and i < len(frame_ids):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count == frame_ids[i]:\n            frame = cv2.resize(frame, img_size)\n            path = os.path.join(output_dir, f\"frame_{count}.jpg\")\n            cv2.imwrite(path, frame)\n            frame_paths.append({'path': path, 'label': os.path.basename(os.path.dirname(video_path))})\n            i += 1\n        count += 1\n    cap.release()\n    return frame_paths\n\n# ✅ Dataset Processing\ndef process_dataset_balanced(real_dir, fake_dir, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    all_frames = []\n\n    real_videos = [f for f in os.listdir(real_dir) if f.endswith(('.mp4', '.avi'))]\n    fake_videos = [f for f in os.listdir(fake_dir) if f.endswith(('.mp4', '.avi'))]\n\n    random.shuffle(real_videos)\n    random.shuffle(fake_videos)\n\n    test_real = real_videos[:10]\n    test_fake = fake_videos[:10]\n    trainval_real = real_videos[10:190]\n    trainval_fake = fake_videos[10:190]\n\n    def process_videos(video_list, label, split):\n        for video in video_list:\n            path = os.path.join(real_dir if label == 'real' else fake_dir, video)\n            out_dir = os.path.join(output_dir, split, label, os.path.splitext(video)[0])\n            all_frames.extend(extract_frames(path, out_dir))\n\n    process_videos(trainval_real, \"real\", \"trainval\")\n    process_videos(trainval_fake, \"fake\", \"trainval\")\n    process_videos(test_real, \"real\", \"test\")\n    process_videos(test_fake, \"fake\", \"test\")\n\n    return pd.DataFrame(all_frames)\n\n# ✅ SE Block\ndef se_block(input_tensor, ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\n# ✅ SE-CNN Model\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(\n        optimizer=Adam(learning_rate=0.0005),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=['accuracy']\n    )\n    return model\n\n# ✅ Video-Level Aggregation\ndef video_level_prediction(frame_preds):\n    return int(np.mean(frame_preds) > 0.5)\n\n# ✅ Main\ndef main():\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset_balanced(real_dir, fake_dir, output_dir)\n    if frame_df.empty:\n        raise ValueError(\"❌ No data processed\")\n\n    trainval_df = frame_df[frame_df['path'].str.contains(\"trainval\")]\n    test_df = frame_df[frame_df['path'].str.contains(\"test\")]\n\n    train_df, val_df = train_test_split(trainval_df, test_size=0.2, stratify=trainval_df['label'])\n    for df in [train_df, val_df, test_df]:\n        df['label'] = df['label'].astype(str)\n\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n    )\n\n    train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label',\n                                            target_size=(96, 96), batch_size=16, class_mode='binary')\n    val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label',\n                                          target_size=(96, 96), batch_size=16, class_mode='binary')\n    test_gen = datagen.flow_from_dataframe(test_df, x_col='path', y_col='label',\n                                           target_size=(96, 96), batch_size=16, class_mode='binary', shuffle=False)\n\n    model = create_se_cnn_model()\n    model.summary()\n\n    callbacks = [\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    ]\n\n    history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=callbacks)\n    model.save(\"/kaggle/working/se_cnn_modified_model.h5\")\n    print(\"✅ Model saved successfully!\")\n\n    # ✅ Evaluate on test set\n    test_loss, test_acc = model.evaluate(test_gen)\n    print(f\"\\n✅ Frame-level Test Accuracy: {test_acc:.4f}\")\n    print(f\"✅ Final Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n    print(f\"✅ Final Val Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n\n    # ✅ Video-level evaluation\n    preds = model.predict(test_gen, verbose=1)\n    paths = test_gen.filenames\n\n    video_preds = defaultdict(list)\n    video_labels = {}\n\n    for i, path in enumerate(paths):\n        parts = path.split('/')\n        label = parts[2]\n        video_id = parts[3]\n        video_key = f\"{label}/{video_id}\"\n        video_preds[video_key].append(preds[i][0])\n        video_labels[video_key] = 1 if label == 'fake' else 0\n\n    correct = 0\n    total = 0\n    for vid, pred_list in video_preds.items():\n        pred_label = video_level_prediction(pred_list)\n        true_label = video_labels[vid]\n        correct += (pred_label == true_label)\n        total += 1\n\n    print(f\"🎬 Video-level Accuracy: {correct}/{total} = {correct / total:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:48:54.527443Z","iopub.execute_input":"2025-04-10T07:48:54.527868Z"}},"outputs":[{"name":"stdout","text":"Found 5760 validated image filenames belonging to 2 classes.\nFound 1440 validated image filenames belonging to 2 classes.\nFound 400 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-826d844febfa>:161: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['label'] = df['label'].astype(str)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m17\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m32\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m128\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m66\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m96\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│                           │                        │                │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m256\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m260\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m320\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│                           │                        │                │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m8,320\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│                           │                        │                │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│                           │                        │                │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,272\u001b[0m (129.97 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,272</span> (129.97 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,048\u001b[0m (129.09 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,048</span> (129.09 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 149ms/step - accuracy: 0.5077 - loss: 0.7088 - val_accuracy: 0.5000 - val_loss: 0.7085 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m235/360\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.5165 - loss: 0.7012","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n    GlobalAveragePooling2D, Input, Reshape, Multiply,\n    BatchNormalization\n)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Dataset folders 'real' and 'fake' not found!\")\n    return real_dir, fake_dir\n\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n    saved_frames = []\n    frame_count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count in frames_to_extract:\n            try:\n                frame = cv2.resize(frame, img_size)\n                frame_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n                cv2.imwrite(frame_path, frame)\n                saved_frames.append(frame_path)\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n        frame_count += 1\n    cap.release()\n    return saved_frames\n\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    frame_data = []\n    def process_video_batch(video_dir, label):\n        batch_frames = []\n        for video_file in os.listdir(video_dir)[:max_videos]:\n            if video_file.endswith(('.mp4', '.avi')):\n                video_path = os.path.join(video_dir, video_file)\n                video_output_dir = os.path.join(output_dir, label, os.path.splitext(video_file)[0])\n                frames = extract_frames(video_path, video_output_dir)\n                for frame in frames:\n                    batch_frames.append({'path': frame, 'label': label})\n                clear_memory()\n        return batch_frames\n    frame_data += process_video_batch(real_dir, \"real\")\n    frame_data += process_video_batch(fake_dir, \"fake\")\n    return pd.DataFrame(frame_data)\n\ndef se_block(input_tensor, reduction_ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // reduction_ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(inputs)\n    x = BatchNormalization()(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(x)\n    x = BatchNormalization()(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=l2(0.002))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n    return model\n\ndef main():\n    print(\"🚀 Starting Deepfake Detection Training...\")\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir, max_videos=50)\n\n    if frame_df.empty:\n        raise ValueError(\"❌ Frame dataset is empty!\")\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n\n    # 🔁 Convert labels to binary (0 = real, 1 = fake)\n    train_df['label'] = (train_df['label'] == 'fake').astype(int)\n    val_df['label'] = (val_df['label'] == 'fake').astype(int)\n\n    print(\"✅ Label distribution:\")\n    print(\"Train:\", train_df['label'].value_counts().to_dict())\n    print(\"Val:\", val_df['label'].value_counts().to_dict())\n\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=15,\n        zoom_range=0.1,\n        brightness_range=[0.8, 1.2],\n        horizontal_flip=True\n    )\n    val_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_dataframe(\n        train_df, x_col='path', y_col='label',\n        target_size=(96, 96), batch_size=16, class_mode='raw', shuffle=True\n    )\n    val_generator = val_datagen.flow_from_dataframe(\n        val_df, x_col='path', y_col='label',\n        target_size=(96, 96), batch_size=16, class_mode='raw', shuffle=False\n    )\n\n    model = create_se_cnn_model()\n    model.summary()\n\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n        ModelCheckpoint(\"/kaggle/working/se_cnn_best_model.h5\", save_best_only=True, verbose=1)\n    ]\n\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=30,\n        callbacks=callbacks\n    )\n\n    model.save(\"/kaggle/working/se_cnn_final_model.h5\")\n    print(\"✅ Model training complete and saved as .h5!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T04:20:46.090804Z","iopub.execute_input":"2025-04-09T04:20:46.091305Z","iopub.status.idle":"2025-04-09T04:27:57.553488Z","shell.execute_reply.started":"2025-04-09T04:20:46.091266Z","shell.execute_reply":"2025-04-09T04:27:57.551053Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Deepfake Detection Training...\n✅ Label distribution:\nTrain: {0: 400, 1: 400}\nVal: {1: 100, 0: 100}\nFound 800 validated image filenames.\nFound 200 validated image filenames.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m17\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m32\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n│                           │                        │                │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m128\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m256\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m260\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m320\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n│                           │                        │                │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m1,179,776\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                           │                        │                │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n│                           │                        │                │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,776</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,204,566\u001b[0m (4.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,204,566</span> (4.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,204,342\u001b[0m (4.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,204,342</span> (4.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n</pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e05a5a45149b>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-e05a5a45149b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/se_cnn_best_model.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     ]\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;34m\"The filepath provided must end in `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;34m\"(Keras model format). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=/kaggle/working/se_cnn_best_model.h5"],"ename":"ValueError","evalue":"The filepath provided must end in `.keras` (Keras model format). Received: filepath=/kaggle/working/se_cnn_best_model.h5","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n    GlobalAveragePooling2D, Input, Reshape, Multiply,\n    BatchNormalization\n)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Dataset folders 'real' and 'fake' not found!\")\n    return real_dir, fake_dir\n\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n    saved_frames = []\n    frame_count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count in frames_to_extract:\n            try:\n                frame = cv2.resize(frame, img_size)\n                frame_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n                cv2.imwrite(frame_path, frame)\n                saved_frames.append(frame_path)\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n        frame_count += 1\n    cap.release()\n    return saved_frames\n\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    frame_data = []\n    def process_video_batch(video_dir, label):\n        batch_frames = []\n        for video_file in os.listdir(video_dir)[:max_videos]:\n            if video_file.endswith(('.mp4', '.avi')):\n                video_path = os.path.join(video_dir, video_file)\n                video_output_dir = os.path.join(output_dir, label, os.path.splitext(video_file)[0])\n                frames = extract_frames(video_path, video_output_dir)\n                for frame in frames:\n                    batch_frames.append({'path': frame, 'label': label})\n                clear_memory()\n        return batch_frames\n    frame_data += process_video_batch(real_dir, \"real\")\n    frame_data += process_video_batch(fake_dir, \"fake\")\n    return pd.DataFrame(frame_data)\n\ndef se_block(input_tensor, reduction_ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // reduction_ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(inputs)\n    x = BatchNormalization()(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.002))(x)\n    x = BatchNormalization()(x)\n    x = se_block(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=l2(0.002))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n    return model\n\ndef main():\n    print(\"🚀 Starting Deepfake Detection Training...\")\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir, max_videos=50)\n\n    if frame_df.empty:\n        raise ValueError(\"❌ Frame dataset is empty!\")\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n\n    train_df['label'] = (train_df['label'] == 'fake').astype(int)\n    val_df['label'] = (val_df['label'] == 'fake').astype(int)\n\n    print(\"✅ Label distribution:\")\n    print(\"Train:\", train_df['label'].value_counts().to_dict())\n    print(\"Val:\", val_df['label'].value_counts().to_dict())\n\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=15,\n        zoom_range=0.1,\n        brightness_range=[0.8, 1.2],\n        horizontal_flip=True\n    )\n    val_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_dataframe(\n        train_df, x_col='path', y_col='label',\n        target_size=(96, 96), batch_size=16, class_mode='raw', shuffle=True\n    )\n    val_generator = val_datagen.flow_from_dataframe(\n        val_df, x_col='path', y_col='label',\n        target_size=(96, 96), batch_size=16, class_mode='raw', shuffle=False\n    )\n\n    model = create_se_cnn_model()\n    model.summary()\n\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n        ModelCheckpoint(\"/kaggle/working/se_cnn_best_weights.h5\", save_best_only=True, save_weights_only=True, verbose=1)\n    ]\n\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=30,\n        callbacks=callbacks\n    )\n\n    model.save(\"/kaggle/working/se_cnn_final_model.h5\")\n    print(\"✅ Model training complete and saved as .h5!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T04:19:11.855852Z","iopub.execute_input":"2025-04-09T04:19:11.856324Z","iopub.status.idle":"2025-04-09T04:19:30.422345Z","shell.execute_reply.started":"2025-04-09T04:19:11.856288Z","shell.execute_reply":"2025-04-09T04:19:30.420825Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model.save('/kaggle/working/se_cnn_best_model.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T04:18:07.933960Z","iopub.execute_input":"2025-04-09T04:18:07.934300Z","iopub.status.idle":"2025-04-09T04:18:08.344112Z","shell.execute_reply.started":"2025-04-09T04:18:07.934274Z","shell.execute_reply":"2025-04-09T04:18:08.342180Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-62458e3d77a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/se_cnn_best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# LSTM ","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\ndef extract_frames(video_path, max_frames=10, img_size=(96, 96)):\n    \"\"\"Extracts frames from a video and returns them as a sequence.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"❌ Error opening video: {video_path}\")\n        return None\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=max_frames, dtype=int)\n    frame_list = []\n\n    for frame_idx in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx in frames_to_extract:\n            frame = cv2.resize(frame, img_size)\n            frame = frame / 255.0  \n            frame_list.append(frame)\n\n    cap.release()\n    return np.array(frame_list)\n\ndef process_dataset(real_dir, fake_dir, max_videos=50):\n    \"\"\"Extracts frames from real and fake videos, saves as dataset.\"\"\"\n    video_sequences = []\n    labels = []\n\n    for video_file in os.listdir(real_dir)[:max_videos]:\n        if video_file.endswith('.mp4'):\n            video_path = os.path.join(real_dir, video_file)\n            frames = extract_frames(video_path)\n            if frames is not None:\n                video_sequences.append(frames)\n                labels.append(0)  \n\n    for video_file in os.listdir(fake_dir)[:max_videos]:\n        if video_file.endswith('.mp4'):\n            video_path = os.path.join(fake_dir, video_file)\n            frames = extract_frames(video_path)\n            if frames is not None:\n                video_sequences.append(frames)\n                labels.append(1)  \n\n    video_sequences = np.array(video_sequences)  \n    labels = np.array(labels)\n\n    np.save(\"/kaggle/working/train_videos.npy\", video_sequences)\n    np.save(\"/kaggle/working/train_labels.npy\", labels)\n\n    print(f\"✅ Dataset saved: {video_sequences.shape}, Labels: {labels.shape}\")\n\nif __name__ == \"__main__\":\n    real_dir, fake_dir = get_dataset_paths()\n    process_dataset(real_dir, fake_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:15:08.394517Z","iopub.status.idle":"2025-04-15T10:15:08.394887Z","shell.execute_reply":"2025-04-15T10:15:08.394755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ✅ Load Dataset\ntrain_videos = np.load(\"/kaggle/working/train_videos.npy\")  # Shape (num_samples, 10, 96, 96, 3)\ntrain_labels = np.load(\"/kaggle/working/train_labels.npy\")\n\n# ✅ Reshape Data for LSTM\nnum_samples, time_steps, height, width, channels = train_videos.shape\nfeature_dim = height * width * channels  \n\ntrain_videos = train_videos.reshape(num_samples, time_steps, feature_dim)  # Flatten each frame\ntrain_videos, val_videos, train_labels, val_labels = train_test_split(train_videos, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n\n# ✅ Define LSTM Model\ndef create_lstm_model(input_shape=(10, 27648)):  # 10 time steps, flattened 96x96x3 frames\n    model = Sequential([\n        LSTM(128, return_sequences=True, input_shape=input_shape),\n        Dropout(0.5),\n        LSTM(64, return_sequences=False),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dense(1, activation='sigmoid')  \n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Train and Evaluate Model\ndef train_lstm_model():\n    model = create_lstm_model()\n\n    history = model.fit(train_videos, train_labels, validation_data=(val_videos, val_labels), epochs=15, batch_size=16)\n\n    predictions = model.predict(val_videos)\n    binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n\n    acc = accuracy_score(val_labels, binary_preds)\n    precision = precision_score(val_labels, binary_preds)\n    recall = recall_score(val_labels, binary_preds)\n    f1 = f1_score(val_labels, binary_preds)\n    cm = confusion_matrix(val_labels, binary_preds)\n\n    print(f\"\\n🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix - LSTM Only\")\n    plt.show()\n\n# ✅ Run Experiment\nif __name__ == \"__main__\":\n    train_lstm_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:19:26.795193Z","iopub.execute_input":"2025-04-15T10:19:26.795530Z","iopub.status.idle":"2025-04-15T10:19:42.934313Z","shell.execute_reply.started":"2025-04-15T10:19:26.795504Z","shell.execute_reply":"2025-04-15T10:19:42.932872Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-97875c8426b8>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# ✅ Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_videos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/train_videos.npy\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape (num_samples, 10, 96, 96, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/train_labels.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/train_videos.npy'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/train_videos.npy'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Bidirectional LSTM\n","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\n# ✅ Extract frames from videos\ndef extract_frames(video_path, num_frames=10, resize=(96, 96)):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    interval = max(1, total_frames // num_frames)\n\n    for i in range(num_frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, resize)\n        frames.append(frame)\n\n    cap.release()\n    return np.array(frames) if len(frames) == num_frames else None\n\n# ✅ Load all videos and prepare data\ndef load_dataset():\n    real_dir, fake_dir = get_dataset_paths()\n    X, y = [], []\n\n    for label, folder in enumerate([real_dir, fake_dir]):\n        for filename in os.listdir(folder):\n            if filename.endswith(\".mp4\"):\n                video_path = os.path.join(folder, filename)\n                frames = extract_frames(video_path)\n                if frames is not None:\n                    X.append(frames)\n                    y.append(label)\n\n    X = np.array(X) / 255.0  # Normalize\n    y = np.array(y)\n    return X, y\n\n# ✅ Reshape for LSTM input\ndef reshape_for_lstm(X):\n    num_samples, time_steps, height, width, channels = X.shape\n    return X.reshape(num_samples, time_steps, height * width * channels)\n\n# ✅ Define Bi-LSTM Model\ndef create_lstm_model(input_shape):\n    model = Sequential([\n        Bidirectional(LSTM(256, return_sequences=True, input_shape=input_shape)),\n        BatchNormalization(),\n        Dropout(0.3),\n\n        Bidirectional(LSTM(128, return_sequences=False)),\n        BatchNormalization(),\n        Dropout(0.3),\n\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Train and Evaluate\ndef train_lstm_model():\n    X, y = load_dataset()\n    X = reshape_for_lstm(X)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n    model = create_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=10)\n\n    preds = model.predict(X_val)\n    preds_binary = [1 if p >= 0.5 else 0 for p in preds]\n\n    acc = accuracy_score(y_val, preds_binary)\n    precision = precision_score(y_val, preds_binary)\n    recall = recall_score(y_val, preds_binary)\n    f1 = f1_score(y_val, preds_binary)\n    cm = confusion_matrix(y_val, preds_binary)\n\n    print(f\"\\n🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix - Bi-LSTM\")\n    plt.show()\n\n# ✅ Run\nif __name__ == \"__main__\":\n    train_lstm_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:19:25.262159Z","iopub.execute_input":"2025-04-15T19:19:25.262471Z","iopub.status.idle":"2025-04-15T19:45:57.571575Z","shell.execute_reply.started":"2025-04-15T19:19:25.262443Z","shell.execute_reply":"2025-04-15T19:45:57.570730Z"}},"outputs":[{"name":"stdout","text":"✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 91ms/step - accuracy: 0.4884 - loss: 1.0422 - val_accuracy: 0.5000 - val_loss: 0.7002\nEpoch 2/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5090 - loss: 0.8466 - val_accuracy: 0.5000 - val_loss: 0.7387\nEpoch 3/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4942 - loss: 0.9056 - val_accuracy: 0.5000 - val_loss: 0.7558\nEpoch 4/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4590 - loss: 0.9175 - val_accuracy: 0.5000 - val_loss: 0.7391\nEpoch 5/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4890 - loss: 0.8298 - val_accuracy: 0.5000 - val_loss: 0.7090\nEpoch 6/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4723 - loss: 0.8797 - val_accuracy: 0.5000 - val_loss: 0.7112\nEpoch 7/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4604 - loss: 0.8343 - val_accuracy: 0.5000 - val_loss: 0.7168\nEpoch 8/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4895 - loss: 0.8328 - val_accuracy: 0.5000 - val_loss: 0.7281\nEpoch 9/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5012 - loss: 0.8041 - val_accuracy: 0.5000 - val_loss: 0.7203\nEpoch 10/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4764 - loss: 0.8220 - val_accuracy: 0.5000 - val_loss: 0.7094\nEpoch 11/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4790 - loss: 0.7995 - val_accuracy: 0.5000 - val_loss: 0.6943\nEpoch 12/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4732 - loss: 0.7979 - val_accuracy: 0.5000 - val_loss: 0.7064\nEpoch 13/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4978 - loss: 0.7679 - val_accuracy: 0.5000 - val_loss: 0.6966\nEpoch 14/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4829 - loss: 0.7570 - val_accuracy: 0.5000 - val_loss: 0.7054\nEpoch 15/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5261 - loss: 0.7457 - val_accuracy: 0.4500 - val_loss: 0.6984\nEpoch 16/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4786 - loss: 0.7544 - val_accuracy: 0.5000 - val_loss: 0.6964\nEpoch 17/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5047 - loss: 0.7295 - val_accuracy: 0.5000 - val_loss: 0.6956\nEpoch 18/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4988 - loss: 0.7469 - val_accuracy: 0.5000 - val_loss: 0.6987\nEpoch 19/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5140 - loss: 0.7332 - val_accuracy: 0.5000 - val_loss: 0.6931\nEpoch 20/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4184 - loss: 0.7822 - val_accuracy: 0.5000 - val_loss: 0.7301\nEpoch 21/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5050 - loss: 0.7478 - val_accuracy: 0.5000 - val_loss: 0.7159\nEpoch 22/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5125 - loss: 0.7223 - val_accuracy: 0.5000 - val_loss: 0.9123\nEpoch 23/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5104 - loss: 0.7253 - val_accuracy: 0.5000 - val_loss: 0.7940\nEpoch 24/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.4962 - loss: 0.7323 - val_accuracy: 0.5000 - val_loss: 0.7331\nEpoch 25/25\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.4859 - loss: 0.7019 - val_accuracy: 0.5000 - val_loss: 0.7078\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step\n\n🔹 Accuracy: 0.5000\n🔹 Precision: 0.5000\n🔹 Recall: 1.0000\n🔹 F1 Score: 0.6667\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# cnn-lstm model","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve\n)\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, \n    Dense, Dropout, LSTM, Bidirectional, TimeDistributed, \n    BatchNormalization, concatenate\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, \n    EarlyStopping, \n    ModelCheckpoint\n)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nclass VideoDataGenerator(Sequence):\n    \"\"\"Advanced data generator for video sequences with augmentation\"\"\"\n    def __init__(self, dataframe, batch_size=32, shuffle=True, augment=True):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.indexes = np.arange(len(self.dataframe))\n        \n        # Image augmentation\n        self.image_datagen = ImageDataGenerator(\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True,\n            zoom_range=0.2,\n            shear_range=0.2\n        ) if augment else None\n        \n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n    \n    def __getitem__(self, idx):\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_df = self.dataframe.iloc[batch_indexes]\n        \n        X_batch = []\n        y_batch = []\n        \n        for _, row in batch_df.iterrows():\n            frames = self.extract_frames(row['video_path'])\n            if frames is not None:\n                # Augment frames if enabled\n                if self.augment and self.image_datagen:\n                    augmented_frames = []\n                    for frame in frames:\n                        # Apply augmentation to each frame\n                        frame = self.image_datagen.random_transform(frame)\n                        augmented_frames.append(frame)\n                    frames = np.array(augmented_frames)\n                \n                X_batch.append(frames)\n                y_batch.append(row['label'])\n        \n        return np.array(X_batch), np.array(y_batch)\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    @staticmethod\n    def extract_frames(video_path, sequence_length=10, img_size=(96, 96)):\n        \"\"\"\n        Extract frames from a video with intelligent sampling and preprocessing\n        \n        Args:\n            video_path (str): Path to the video file\n            sequence_length (int): Number of frames to extract\n            img_size (tuple): Desired frame resize dimensions\n        \n        Returns:\n            np.array: Normalized video frames\n        \"\"\"\n        try:\n            cap = cv2.VideoCapture(video_path)\n            if not cap.isOpened():\n                print(f\"Error opening video file: {video_path}\")\n                return None\n\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            # Intelligent frame sampling\n            frame_indices = np.linspace(0, total_frames - 1, sequence_length).astype(int)\n\n            frames = []\n            for frame_count in range(total_frames):\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                if frame_count in frame_indices:\n                    # Resize and preprocess frame\n                    frame = cv2.resize(frame, img_size)\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    \n                    # Normalize and convert to float\n                    frame = frame.astype(np.float32) / 255.0\n                    \n                    frames.append(frame)\n\n            cap.release()\n\n            # Handle videos shorter than sequence length\n            if len(frames) < sequence_length:\n                # Pad with last frame\n                frames += [frames[-1]] * (sequence_length - len(frames))\n            \n            return np.array(frames)\n        \n        except Exception as e:\n            print(f\"Error processing {video_path}: {e}\")\n            return None\n\ndef create_advanced_cnn_lstm_model(input_shape=(10, 96, 96, 3)):\n    \"\"\"\n    Create an advanced CNN-LSTM hybrid model for deepfake detection\n    \n    Args:\n        input_shape (tuple): Shape of input video frames\n    \n    Returns:\n        tf.keras.Model: Compiled deepfake detection model\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    \n    # Spatial feature extraction\n    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(inputs)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(GlobalAveragePooling2D())(x)\n    \n    # Temporal feature extraction\n    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(x)\n    x = Bidirectional(LSTM(64, dropout=0.3))(x)\n    \n    # Classification head\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    \n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Advanced optimizer with custom learning rate\n    optimizer = Adam(\n        learning_rate=1e-4, \n        beta_1=0.9, \n        beta_2=0.999, \n        epsilon=1e-8\n    )\n    \n    model.compile(\n        optimizer=optimizer, \n        loss='binary_crossentropy', \n        metrics=['accuracy', 'precision', 'recall']\n    )\n    \n    return model\n\ndef find_dataset_paths(base_paths=None):\n    \"\"\"\n    Dynamically find dataset paths\n    \n    Args:\n        base_paths (list): Optional list of paths to search\n    \n    Returns:\n        tuple: Paths to real and fake video directories\n    \"\"\"\n    if base_paths is None:\n        base_paths = [\n            '/kaggle/input/faceforensics',\n            '/kaggle/input/faceforensics/FF++',\n            '/kaggle/input',\n            '/content',\n            '.'\n        ]\n    \n    for base_path in base_paths:\n        # Check multiple possible dataset structures\n        dataset_paths = [\n            os.path.join(base_path, 'real'),\n            os.path.join(base_path, 'fake'),\n            os.path.join(base_path, 'FF++', 'real'),\n            os.path.join(base_path, 'FF++', 'fake')\n        ]\n        \n        for real_dir in dataset_paths:\n            for fake_dir in dataset_paths:\n                if real_dir != fake_dir and os.path.exists(real_dir) and os.path.exists(fake_dir):\n                    print(f\"🎉 Dataset found!\")\n                    print(f\"   Real videos: {real_dir}\")\n                    print(f\"   Fake videos: {fake_dir}\")\n                    return real_dir, fake_dir\n    \n    raise FileNotFoundError(\"No dataset found. Please provide valid paths.\")\n\ndef process_dataset(real_dir, fake_dir, max_videos=None):\n    \"\"\"\n    Process video dataset\n    \n    Args:\n        real_dir (str): Directory containing real videos\n        fake_dir (str): Directory containing fake videos\n        max_videos (int, optional): Maximum number of videos to process\n    \n    Returns:\n        pd.DataFrame: Processed dataset\n    \"\"\"\n    def process_directory(directory, label):\n        videos = []\n        for video_file in os.listdir(directory)[:max_videos]:\n            video_path = os.path.join(directory, video_file)\n            videos.append({'video_path': video_path, 'label': label})\n        return videos\n    \n    # Process real and fake videos\n    real_videos = process_directory(real_dir, 0)\n    fake_videos = process_directory(fake_dir, 1)\n    \n    # Combine and shuffle\n    df = pd.DataFrame(real_videos + fake_videos)\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    return df\n\ndef plot_training_history(history):\n    \"\"\"\n    Visualize model training history\n    \n    Args:\n        history (tf.keras.callbacks.History): Training history object\n    \"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # Loss subplot\n    plt.subplot(1, 3, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy subplot\n    plt.subplot(1, 3, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Precision and Recall\n    plt.subplot(1, 3, 3)\n    plt.plot(history.history['precision'], label='Training Precision')\n    plt.plot(history.history['val_precision'], label='Validation Precision')\n    plt.plot(history.history['recall'], label='Training Recall')\n    plt.plot(history.history['val_recall'], label='Validation Recall')\n    plt.title('Precision & Recall')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef evaluate_model(model, val_generator):\n    \"\"\"\n    Comprehensive model evaluation\n    \n    Args:\n        model (tf.keras.Model): Trained model\n        val_generator (VideoDataGenerator): Validation data generator\n    \n    Returns:\n        dict: Evaluation metrics\n    \"\"\"\n    # Collect true labels and predictions\n    y_true = []\n    y_pred_proba = []\n    \n    for X, y in val_generator:\n        predictions = model.predict(X)\n        y_true.extend(y)\n        y_pred_proba.extend(predictions)\n    \n    # Convert probabilities to binary predictions\n    y_pred = (np.array(y_pred_proba) > 0.5).astype(int)\n    \n    # Classification Report\n    print(\"\\n📊 Detailed Classification Report:\")\n    report = classification_report(y_true, y_pred, target_names=['Real', 'Fake'])\n    print(report)\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n    \n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return {\n        'classification_report': report,\n        'confusion_matrix': cm,\n        'roc_auc': roc_auc\n    }\n\ndef train_deepfake_model(\n    test_size=0.2, \n    max_videos=None, \n    epochs=50, \n    batch_size=32\n):\n    \"\"\"\n    Train deepfake detection model\n    \n    Args:\n        test_size (float): Proportion of dataset for validation\n        max_videos (int): Maximum number of videos to process\n        epochs (int): Number of training epochs\n        batch_size (int): Training batch size\n    \n    Returns:\n        tuple: Trained model, training history, evaluation metrics\n    \"\"\"\n    # Find dataset paths\n    real_dir, fake_dir = find_dataset_paths()\n    \n    # Process dataset\n    print(\"🚀 Processing Dataset...\")\n    df = process_dataset(real_dir, fake_dir, max_videos)\n    \n    # Split dataset\n    train_df, val_df = train_test_split(\n        df, \n        test_size=test_size, \n        stratify=df['label'], \n        random_state=42\n    )\n    \n    # Create data generators\n    train_generator = VideoDataGenerator(\n        train_df, \n        batch_size=batch_size, \n        shuffle=True, \n        augment=True\n    )\n    val_generator = VideoDataGenerator(\n        val_df, \n        batch_size=batch_size, \n        shuffle=False, \n        augment=False\n    )\n    \n    # Model creation\n    model = create_advanced_cnn_lstm_model()\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss', \n        patience=10, \n        restore_best_weights=True\n    )\n    \n    lr_reducer = ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.5, \n        patience=5, \n        min_lr=1e-6\n    )\n    \n    model_checkpoint = ModelCheckpoint(\n        'best_model.keras',  # Changed file extension to .keras\n        save_best_only=True, \n        monitor='val_accuracy'\n    )\n    \n    # Training\n    print(\"🚀 Training Model...\")\n    history = model.fit(\n        train_generator, \n        validation_data=val_generator, \n        epochs=epochs,\n        callbacks=[early_stopping, lr_reducer, model_checkpoint]\n    )\n    \n    # Visualization\n    plot_training_history(history)\n    \n    # Evaluation\n    evaluation_metrics = evaluate_model(model, val_generator)\n    \n    return model, history, evaluation_metrics\n\ndef predict_video(model, video_path):\n    \"\"\"\n    Predict whether a single video is real or fake\n    \n    Args:\n        model (tf.keras.Model): Trained deepfake detection model\n        video_path (str): Path to video file to predict\n    \n    Returns:\n        float: Probability of being a fake video\n    \"\"\"\n    # Extract frames\n    frames = VideoDataGenerator.extract_frames(video_path)\n    \n    if frames is not None:\n        # Add batch dimension\n        frames = np.expand_dims(frames, axis=0)\n        prediction = model.predict(frames)[0][0]\n        \n        # Interpret prediction\n        label = \"Fake\" if prediction > 0.5 else \"Real\"\n        confidence = prediction if prediction > 0.5 else 1 - prediction\n        \n        print(f\"🎬 Video Analysis:\")\n        print(f\"Predicted Label: {label}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        return prediction\n    \n    print(\"Could not process video.\")\n    return None\n\ndef main():\n    # Train the model\n    model, history, metrics = train_deepfake_model()\n    \n    # Optional: Predict a specific video\n    # Replace with an actual video path from your dataset\n    # sample_video_path = '/path/to/your/video.mp4'\n    # predict_video(model, sample_video_path)\n    \n    return model, history, metrics\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:09:02.764172Z","iopub.execute_input":"2025-04-16T13:09:02.764508Z"}},"outputs":[{"name":"stdout","text":"🎉 Dataset found!\n   Real videos: /kaggle/input/faceforensics/FF++/real\n   Fake videos: /kaggle/input/faceforensics/FF++/fake\n🚀 Processing Dataset...\n🚀 Training Model...\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1390s\u001b[0m 67s/step - accuracy: 0.4663 - loss: 0.7106 - precision: 0.4622 - recall: 0.8833 - val_accuracy: 0.5000 - val_loss: 0.6910 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1174s\u001b[0m 46s/step - accuracy: 0.4805 - loss: 0.7014 - precision: 0.4910 - recall: 0.7468 - val_accuracy: 0.5500 - val_loss: 0.6933 - val_precision: 0.5263 - val_recall: 1.0000 - learning_rate: 1.0000e-04\nEpoch 3/50\n","output_type":"stream"}],"execution_count":null}]}